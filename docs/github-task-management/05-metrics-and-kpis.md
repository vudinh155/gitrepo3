# 05 - Metrics and KPIs (H·ªá th·ªëng ch·ªâ s·ªë)

> **M·ª•c ti√™u**: Hi·ªÉu v√† s·ª≠ d·ª•ng ƒë√∫ng metrics ƒë·ªÉ ƒëo l∆∞·ªùng

**Th·ªùi l∆∞·ª£ng**: 60 ph√∫t
**ƒê·ªëi t∆∞·ª£ng**: PO, HR, DM

---

## ‚ö†Ô∏è C·∫¢NH B√ÅO QUAN TR·ªåNG

```
‚ùå KH√îNG chase s·ªë l∆∞·ª£ng:
   - Commit count
   - PR count
   - Lines of code

‚úÖ FOCUS v√†o:
   - Impact (t√°c ƒë·ªông)
   - Quality (ch·∫•t l∆∞·ª£ng)
   - Collaboration (ph·ªëi h·ª£p)
```

---

## üìä Metrics Categories

### 1Ô∏è‚É£ **Individual Metrics** (C√° nh√¢n)

### A. **Productivity Metrics**

| Metric | Formula | √ù nghƒ©a | Target |
|--------|---------|---------|--------|
| **Issues Completed** | Count(issues closed by user) | S·ªë task ho√†n th√†nh | Track trend |
| **Story Points Delivered** | Sum(points of completed issues) | Velocity c√° nh√¢n | Stable ¬±20% |
| **PR Merge Rate** | Merged PRs / Total PRs | T·ªâ l·ªá PR ƒë∆∞·ª£c merge | > 80% |
| **Cycle Time** | Avg(Done date - Start date) | Th·ªùi gian ho√†n th√†nh task | < 5 days |

---

### B. **Quality Metrics**

| Metric | Formula | √ù nghƒ©a | Target |
|--------|---------|---------|--------|
| **Rework Rate** | PRs with requested changes / Total PRs | T·ªâ l·ªá ph·∫£i l√†m l·∫°i | < 30% |
| **Bug Rate** | Bugs created by user / Total issues | T·ªâ l·ªá t·∫°o bug | < 15% |
| **First-time Approval** | PRs approved first time / Total PRs | PR quality | > 70% |
| **Code Churn** | (Lines added + deleted) / Lines final | Code stability | < 2.0 |

---

### C. **Collaboration Metrics**

| Metric | Formula | √ù nghƒ©a | Target |
|--------|---------|---------|--------|
| **Reviews Given** | Count(reviews by user) | ƒê√≥ng g√≥p review | Min 10/sprint |
| **Review Quality** | Helpful comments / Total comments | Ch·∫•t l∆∞·ª£ng review | Track |
| **Response Time** | Avg time to respond to mentions | Responsiveness | < 4h |
| **Pair Programming** | Hours pair coding | Collaboration | Track |

---

## üìà C√°ch T√≠nh Metrics

### Example 1: Issues Completed

**C√°ch l·∫•y data:**

```
GitHub API:
GET /repos/:owner/:repo/issues?state=closed&assignee=username&since=2024-06-01

Ho·∫∑c GitHub Projects:
Filter: Status = Done AND Assignee = @username AND Closed >= Sprint start

Manual count:
V√†o Profile ‚Üí Contributions ‚Üí Issues closed
```

**Interpretation:**

```
Dev Alice (Sprint 15):
  Issues completed: 8

Context needed:
  - Type: 5 features, 2 bugs, 1 task
  - Complexity: Avg 5 points/issue (40 points total)
  - Module: 6 core backend, 2 simple frontend

‚Üí Good productivity v·ªõi high complexity
```

---

### Example 2: Rework Rate

**Formula:**

```
Rework Rate = (PRs with "Request Changes") / (Total PRs created) √ó 100%
```

**C√°ch t√≠nh:**

```
Dev Bob (1 th√°ng):
  Total PRs: 20
  Approved first time: 14
  Request changes: 6

  Rework Rate = 6/20 √ó 100% = 30%

Benchmark:
  - < 20%: Excellent
  - 20-30%: Good
  - 30-50%: Needs improvement
  - > 50%: Serious quality issues
```

---

### Example 3: Review Contribution

**C√°ch t√≠nh:**

```
Reviews Given = Count(PRs reviewed by user)

Dev Carol (1 th√°ng):
  - PRs reviewed: 25
  - Comments: 120
  - Avg comments/review: 4.8
  - Helpful votes: 90 (75% helpful)

‚Üí Active reviewer, high-quality feedback
```

---

## üéØ Individual Performance Dashboard

### Template Dashboard (per person, per sprint)

```markdown
## Dev Alice - Sprint 15 Performance

### Productivity
- Issues completed: 8 (Target: 6-10)
- Story points: 42 (Velocity: stable)
- Cycle time: 3.5 days (Target: < 5)

### Quality
- Rework rate: 15% (Good)
- Bug rate: 10% (Good)
- First-time approval: 85% (Excellent)

### Collaboration
- Reviews given: 12 (Good)
- Comments: 45 (Helpful: 80%)
- Pair programming: 8 hours

### Notable
- Handled core auth module migration (high complexity)
- Mentored junior dev Bob
- Zero production bugs
```

---

## üìä Team Metrics

### 2Ô∏è‚É£ **Team-level Metrics**

| Metric | Formula | √ù nghƒ©a | Target |
|--------|---------|---------|--------|
| **Sprint Velocity** | Sum(story points completed) | Team capacity | Stable |
| **Sprint Completion** | Issues done / Planned | Predictability | > 80% |
| **Lead Time** | Time from issue create ‚Üí deployed | Efficiency | < 2 weeks |
| **Deployment Frequency** | Deploys per week | Agility | > 3/week |
| **Bug Escape Rate** | Prod bugs / Total bugs | Quality gate | < 10% |

---

## ‚ö†Ô∏è Metrics Anti-patterns

### ‚ùå Anti-pattern 1: Commit Count

```
BAD:
  Dev A: 200 commits
  Dev B: 50 commits
  Conclusion: Dev A productive 4x

REALITY:
  Dev A: Commits m·ªói small change, nhi·ªÅu fix commits
  Dev B: Commits atomic, meaningful

  Dev A: Churn rate 3.5 (vi·∫øt r·ªìi x√≥a nhi·ªÅu)
  Dev B: Churn rate 0.8 (stable code)

‚Üí Dev B quality t·ªët h∆°n
```

---

### ‚ùå Anti-pattern 2: Lines of Code

```
BAD:
  Dev A: 5000 lines added
  Dev B: 500 lines added
  Conclusion: Dev A productive 10x

REALITY:
  Dev A: Generated code, boilerplate
  Dev B: Core algorithm, high complexity

  Impact:
    Dev A: Low (replaceable code)
    Dev B: High (critical business logic)

‚Üí Lines of code ‚â† productivity
```

---

### ‚ùå Anti-pattern 3: So s√°nh Cross-role

```
BAD:
  Frontend Dev: 20 PRs/month
  Backend Dev: 10 PRs/month
  Conclusion: Frontend dev 2x productive

REALITY:
  Frontend: Nhi·ªÅu UI changes, nh·ªè, isolated
  Backend: √çt PRs nh∆∞ng complex, critical

‚Üí KH√îNG th·ªÉ so s√°nh tr·ª±c ti·∫øp
```

---

## ‚úÖ C√°ch d√πng Metrics ƒê√öNG

### 1. **Metrics + Context**

```
‚úÖ ƒê√öNG:

Dev performance review:
  1. Metrics (60%):
     - 8 issues completed (40 points)
     - Rework rate: 15%
     - Reviews: 12

  2. Tech Lead input (30%):
     - Handled core module migration
     - Mentored 2 juniors
     - Quality: Excellent

  3. Team feedback (10%):
     - Helpful reviewer
     - Good collaborator

‚Üí Holistic evaluation
```

---

### 2. **Trend > Absolute number**

```
‚úÖ ƒê√öNG:

Sprint 12: 30 points
Sprint 13: 32 points
Sprint 14: 31 points
Sprint 15: 29 points

‚Üí Velocity STABLE (good)

‚ùå SAI:
Sprint 15: 29 points < Sprint 13: 32 points
‚Üí Performance gi·∫£m (sai, v√¨ fluctuation t·ª± nhi√™n)
```

---

### 3. **Relative to team average**

```
‚úÖ ƒê√öNG:

Team average velocity: 35 points/sprint

Dev A: 40 points (Above average)
Dev B: 35 points (Average)
Dev C: 25 points (Below average)

But context:
  Dev C: Junior (6 months exp) + handled complex module
  ‚Üí Actually good for level

‚Üí Compare with context
```

---

## üìä Metrics Collection Tools

### Option 1: GitHub Insights (Built-in)

```
Repository ‚Üí Insights ‚Üí Contributors

Metrics available:
  - Commits
  - Code frequency
  - Additions/Deletions
```

**Pros**: Free, built-in
**Cons**: Limited, no custom metrics

---

### Option 2: GitHub API + Custom Dashboard

```python
# Example: Get issues completed by user

import requests

def get_user_issues_completed(org, repo, username, since_date):
    url = f"https://api.github.com/repos/{org}/{repo}/issues"
    params = {
        "state": "closed",
        "assignee": username,
        "since": since_date
    }
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}

    response = requests.get(url, params=params, headers=headers)
    issues = response.json()

    return len(issues)

# Usage
completed = get_user_issues_completed("myorg", "myrepo", "alice", "2024-06-01")
print(f"Alice completed {completed} issues")
```

---

### Option 3: Third-party Tools

```
- LinearB (metrics + insights)
- Waydev (engineering analytics)
- Pluralsight Flow (team performance)
- Jellyfish (engineering management platform)
```

---

## üìà Monthly Report Template

```markdown
## Team Performance - June 2024

### Team Velocity
- Avg velocity: 180 points/sprint (stable)
- Sprint completion: 85% (target: 80%)
- Lead time: 8 days (target: < 10)

### Top Performers
1. Alice: 48 pts, 0 bugs, 15 reviews (Core contributor)
2. Bob: 42 pts, 1 bug, 12 reviews (Solid performer)
3. Carol: 38 pts, 0 bugs, 20 reviews (Reviewer champion)

### Areas of Improvement
- Bug escape rate: 12% (target: < 10%)
- Action: Strengthen QA process

### Team Highlights
- Launched Payment v2 (5 sprints)
- Zero production incidents
- 3 new features delivered
```

---

## ‚úÖ Checklist sau khi ƒë·ªçc xong

```markdown
- [ ] Hi·ªÉu 3 lo·∫°i metrics: Productivity, Quality, Collaboration
- [ ] Bi·∫øt c√°ch t√≠nh metrics c∆° b·∫£n
- [ ] Hi·ªÉu anti-patterns: commit count, LOC, cross-role comparison
- [ ] Bi·∫øt c√°ch d√πng metrics + context
- [ ] Bi·∫øt tools ƒë·ªÉ collect metrics
```

---

**üöÄ Ti·∫øp theo: [07-fair-evaluation-guidelines.md](./07-fair-evaluation-guidelines.md) - ƒê√°nh gi√° c√¥ng b·∫±ng**
